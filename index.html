<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Guardrails and Security for LLMs: Safe, Secure, and Controllable Steering of LLM Applications @ ACL 2025 tutorial.">
  <meta name="keywords" content="Guardrails, Security, LLM, Safety, Steering">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Guardrails and Security for LLMs: Safe, Secure, and Controllable Steering of LLM Applications</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-WYPGDC9FY7"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-WYPGDC9FY7');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title is-3 publication-title">ACL 2025 Tutorial</h2>
          <h1 class="title is-2 publication-title">Guardrails and Security for LLMs:<br />
            <span style="font-size: 80%">Safe, Secure, and Controllable Steering of LLM Applications</span></h1>
          <div class="is-size-5 publication-authors">
            Presenters:
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/leondz">Leon Derczynski</a><sup>1,6</sup>,
            </span>
            <span class="author-block">
              <a href="https://liweijiang.me/">Liwei Jiang</a><sup>2,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=1BGJ0YMAAAAJ&hl=en">Makesh Narsimhan Sreedhar</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=yrguxEsAAAAJ&hl=en">Prasoon Varshney</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=7NxaE1MAAAAJ&hl=en">Traian Rebedea</a><sup>1,5</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~yuliats/">Yulia Tsvetkov</a><sup>3</sup>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            Other contributors:
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.co.in/citations?user=VZa_QYAAAAAJ&hl=en">Shaona Ghosh</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://fabrahman.github.io/">Faeze Brahman</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://aisecure.github.io/">Bo Li</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.ca/citations?user=kcSSl_YAAAAJ&hl=en">Christopher Parisien</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a><sup>1,3</sup>
            </span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>NVIDIA,</span>
            <span class="author-block"><sup>2</sup>Allen Institute for AI,</span>
            <span class="author-block"><sup>3</sup>University of Washington,</span>
            <span class="author-block"><sup>4</sup>University of Illinois at Urbana-Champaign,</span>
            <span class="author-block"><sup>5</sup>University Politehnica of Bucharest,</span>
            <span class="author-block"><sup>6</sup>ITU University of Copenhagen</span>
          </div>
          <br />
          <div class="is-size-5 publication-authors">
            <b>Sunday July 27 14:00 - 17:30 (CET) @ Austria Center Vienna, Hall C</b>
          </div>

<!--          <div class="column has-text-centered">-->
<!--            <div class="publication-links">-->
<!--              &lt;!&ndash; PDF Link. &ndash;&gt;-->
<!--              <span class="link-block">-->
<!--                <a href="https://arxiv.org/pdf/2011.12948"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fas fa-file-pdf"></i>-->
<!--                  </span>-->
<!--                  <span>Paper</span>-->
<!--                </a>-->
<!--              </span>-->
<!--              <span class="link-block">-->
<!--                <a href="https://arxiv.org/abs/2011.12948"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="ai ai-arxiv"></i>-->
<!--                  </span>-->
<!--                  <span>arXiv</span>-->
<!--                </a>-->
<!--              </span>-->
<!--              &lt;!&ndash; Video Link. &ndash;&gt;-->
<!--              <span class="link-block">-->
<!--                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-youtube"></i>-->
<!--                  </span>-->
<!--                  <span>Video</span>-->
<!--                </a>-->
<!--              </span>-->
<!--              &lt;!&ndash; Code Link. &ndash;&gt;-->
<!--              <span class="link-block">-->
<!--                <a href="https://github.com/google/nerfies"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-github"></i>-->
<!--                  </span>-->
<!--                  <span>Code</span>-->
<!--                  </a>-->
<!--              </span>-->
<!--              &lt;!&ndash; Dataset Link. &ndash;&gt;-->
<!--              <span class="link-block">-->
<!--                <a href="https://github.com/google/nerfies/releases/tag/0.1"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="far fa-images"></i>-->
<!--                  </span>-->
<!--                  <span>Data</span>-->
<!--                  </a>-->
<!--            </div>-->

<!--          </div>-->
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">About this tutorial</h2>
        <div class="content has-text-justified">
          <p>
            Pretrained generative models, especially large language models, provide novel ways for users to interact with computers. While generative NLP research and applications had previously aimed at very domain-specific or task-specific solutions, current LLMs and applications (e.g. dialogue systems, agents) are versatile across many tasks and domains. Despite being trained to be helpful and aligned with human preferences (e.g., harmlessness), enforcing robust guardrails on LLMs remains a challenge. And, even when protected against rudimentary attacks, just like other complex software, LLMs can be vulnerable to attacks using sophisticated adversarial inputs.
          </p>
          <p>
            This tutorial provides a comprehensive overview of key guardrail mechanisms developed for LLMs, along with evaluation methodologies and a detailed security assessment protocol - including auto red-teaming of LLM-powered applications. Our aim is to move beyond the discussion of single prompt attacks and evaluation frameworks towards addressing how guardrailing can be done in complex dialogue systems that employ LLMs.
          </p>
          <p>
            We aim to provide a cutting-edge and complete overview of deployment risks associated with LLMs in production environments. While the main focus will be on how to effectively protect against safety and security threats, we also tackle the more recent topic of providing dialogue and topical rails, including respecting custom policies. We also examine the new attack vectors introduced by LLM-enabled dialogue systems, such as methods for circumventing dialogue steering.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Schedule. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Schedule (tentative)</h2>

        <!-- Schedule table. -->
        <div class="content has-text-justified">
          <!-- Schedule table style used by the ACL 2023 tutorial on LM Retrieval: https://acl2023-retrieval-lm.github.io/ -->
          <style type="text/css">
          .tg  {border-collapse:collapse;border-spacing:0;}
          .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            overflow:hidden;padding:10px 5px;word-break:normal;}
          .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
          .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
          .tg .tg-0lax{text-align:left;vertical-align:top}
          </style>
          <table class="tg">
  <thead>
    <tr>
      <th style="text-align: left;"><strong>Tutorial topic</strong></th>
      <th style="text-align: right;"><strong>Duration</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td class="tg-0lax">Introduction [<a href="./presentations/ACL_2025_Tutorial_LLM_Guardrails_Security_Introduction.pdf" target="_blank" rel="noopener">slides</a>]</td>
      <td class="tg-0lax" style="text-align: right;">5 min</td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Types of LLM guardrails</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Guardrails and LLM security</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">Content moderation and safety</td>
      <td class="tg-0lax" style="text-align: right;">35 min</td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Taxonomies of safety risks</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Landscape of safety models and datasets</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Synthetic data generation for LLM safety</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Custom safety policies</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Safety and reasoning models</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; System level considerations</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">LLM security</td>
      <td class="tg-0lax" style="text-align: right;">30 min</td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Overview</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Tools for assessing LLM security</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Auto red-teaming</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Adversarial attacks</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">Alignment attacks</td>
      <td class="tg-0lax" style="text-align: right;">20 min</td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Data poisoning and sleeper agents</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Instruction hierarchy</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Trojan horse and safety backdoors</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax"><em>Coffee break (3:30-4pm CET)</em></td>
      <td class="tg-0lax" style="text-align: right;"><em>30 min</em></td>
    </tr>
    <tr>
      <td class="tg-0lax">Dialogue rails and security [<a href="./presentations/ACL_2025_Tutorial_LLM_Guardrails_Security_Dialogue_Rails.pdf" target="_blank" rel="noopener">slides</a>]</td>
      <td class="tg-0lax" style="text-align: right;">20 min</td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Dialogue and topical rails</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Evaluation of dialogue rails</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Multi-turn/dialogue attacks and protection</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">Multilingual guardrails [<a href="./presentations/ACL_2025_Tutorial_LLM_Guardrails_Security_Multilingual_Safety.pdf" target="_blank" rel="noopener">slides</a>]</td>
      <td class="tg-0lax" style="text-align: right;">15 min</td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Multilingual safety models</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">Inference-time steering for safety  [<a href="./presentations/ACL_2025_Tutorial_LLM_Guardrails_Security_Inference_Steering.pdf" target="_blank" rel="noopener">slides</a>]</td>
      <td class="tg-0lax" style="text-align: right;">20 min</td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Activation-based steering</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Circuit breakers</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Inference-time steering for concept / topical guardrails</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">LLM agent safety</td>
      <td class="tg-0lax" style="text-align: right;">30 min</td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Safety challenges and measures for different types of basic agents</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Assessing agent safety</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Multi-agent safety risks</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">&emsp; Multi-agents for enhancing AI safety</td>
      <td class="tg-0lax"></td>
    </tr>
    <tr>
      <td class="tg-0lax">Final recommendations</td>
      <td class="tg-0lax" style="text-align: right;">5 min</td>
    </tr>
    <tr>
      <td class="tg-0lax"><strong>Total</strong></td>
      <td class="tg-0lax" style="text-align: right;"><strong>180 min</strong></td>
    </tr>
  </tbody>
</table>

        </div>
        <!--/ Schedule table. -->
      </div>
    </div>
    <!--/ Schedule. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Reading List</h2>

        <div class="content has-text-justified">
          <p>
            <b>Bold papers</b> are the suggested reading list.
          </p>
            <h3 class="title is-5">Content moderation and safety</h3>
            <ul class="is-7">
              <li><b><a href="https://arxiv.org/abs/2406.17864" target="_blank" rel="noopener">Ai risk categorization decoded (air 2024): From government regulations to corporate policies (Zeng et al., 2024)</a></b></li>
              <li><b><a href="https://arxiv.org/abs/2406.18495" target="_blank" rel="noopener">Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms (Han et al., 2024)</a></b></li>
              <li><b><a href="https://arxiv.org/abs/2501.09004" target="_blank" rel="noopener">AEGIS2. 0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails (Ghosh et al., 2025)</a></b></li>
              <li><b><a href="https://arxiv.org/abs/2410.08968" target="_blank" rel="noopener">Controllable safety alignment: Inference-time adaptation to diverse safety requirements (Zhang et al., 2024)</a></b></li>
              <li><a href="https://arxiv.org/abs/2503.06550" target="_blank" rel="noopener">Bingoguard: Llm content moderation tools with risk levels (Yin et al., 2025)</a></li>
              <li><a href="https://arxiv.org/abs/2504.04377" target="_blank" rel="noopener">Polyguard: A multilingual safety moderation tool for 17 languages (Kumar et al., 2025)</a></li>
            </ul>

            <h3 class="title is-5">Multilingual guardrails</h3>
            <ul class="is-7">
              <li><b><a href="https://arxiv.org/abs/2505.24119" target="_blank" rel="noopener">The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It (Yong et al., 2025)</a></b></li>
              <li><b><a href="https://arxiv.org/abs/2505.23856" target="_blank" rel="noopener">OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities (Verma et al., 2025)</a></b></li>
              <li><b><a href="https://arxiv.org/abs/2504.04377" target="_blank" rel="noopener">PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages (Kumar et al., 2025)</a></b></li>
              <li><a href="https://arxiv.org/abs/2406.15948" target="_blank" rel="noopener">Teaching LLMs to Abstain across Languages via Multilingual Feedback (Feng et al., 2024)</a></li>
              <li><a href="https://arxiv.org/abs/2505.16869" target="_blank" rel="noopener">MPO: Multilingual Safety Alignment via Reward Gap Optimization (Zhao et al., 2025)</a></li>
            </ul>

            <h3 class="title is-5">Inference-time steering for safety</h3>
            <ul class="is-7">
              <li><b><a href="https://arxiv.org/abs/2401.11206" target="_blank" rel="noopener">InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance (Wang et al., 2024)</a></b></li>
              <li><b><a href="https://arxiv.org/abs/2410.01174v1" target="_blank" rel="noopener">Towards Inference-time Category-wise Safety Steering for Large Language Models (Bhattacharjee et al., 2024)</a></b></li>
              <li><b><a href="https://arxiv.org/abs/2406.04313" target="_blank" rel="noopener">Improving alignment and robustness with circuit breakers (Zou et al., 2024)</a></b></li>
              <li><b><a href="https://arxiv.org/abs/2411.11296" target="_blank" rel="noopener">Steering Language Model Refusal with Sparse Autoencoders (Oâ€™Brien et al., 2024)</a></b></li>
              <li><a href="https://arxiv.org/pdf/2501.17148" target="_blank" rel="noopener">AXBENCH: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders (Wu et al., 2025)</a></li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<!--<section class="section" id="BibTeX">-->
<!--  <div class="container is-max-desktop content">-->
<!--    <h2 class="title">BibTeX</h2>-->
<!--    <pre><code>@article{park2021nerfies,-->
<!--  author    = {author = {Rebedea, Traian and Derczynski, Leon and Ghosh, Shaona and Sreedhar, Makesh Narsimhan and Brahman, Faeze and Jiang, Liwei and Li, Bo and Tsvetkov, Yulia and Parisien, Christopher and Choi, Yejin}},-->
<!--  title     = {ACL 2025 Tutorial - Guardrails and Security for LLMs: Safe, Secure, and Controllable Steering of LLM Applications},-->
<!--  journal   = {ACL},-->
<!--  year      = {2025},-->
<!--}</code></pre>-->
<!--  </div>-->
<!--</section>-->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="#">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/trebedea" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
